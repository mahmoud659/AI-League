# -*- coding: utf-8 -*-
"""Tatical_Map.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10jrpUl-V9jIjDLMqBKMXshg3q-dityXZ

# Loading Dependencies
"""

!pip install inference

!pip install -q git+https://github.com/roboflow/sports.git

!pip uninstall -y supervision && pip install -q supervision>=0.23.0

"""# Loading Player Detection Model from Yolo"""

from inference import get_model
from google.colab import userdata

ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')
PLAYER_DETECTION_MODEL_ID = "football-players-detection-3zvbc/12"
PLAYER_DETECTION_MODEL = get_model(model_id=PLAYER_DETECTION_MODEL_ID, api_key=ROBOFLOW_API_KEY)

"""# Team Classifying"""

from tqdm import tqdm
import supervision as sv
SOURCE_VIDEO_PATH = "/content/drive/MyDrive/Ai League/video.mp4"
PLAYER_ID = 2
STRIDE = 30

frame_generator = sv.get_video_frames_generator(
    source_path=SOURCE_VIDEO_PATH, stride=STRIDE)

crops = []
for frame in tqdm(frame_generator, desc='collecting crops'):
    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
    detections = sv.Detections.from_inference(result)
    detections = detections.with_nms(threshold=0.5, class_agnostic=True)
    detections = detections[detections.class_id == PLAYER_ID]
    players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]
    crops += players_crops

import torch
from transformers import AutoProcessor, SiglipVisionModel

SIGLIP_MODEL_PATH = 'google/siglip-base-patch16-224'

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
EMBEDDINGS_MODEL = SiglipVisionModel.from_pretrained(SIGLIP_MODEL_PATH).to(DEVICE)
EMBEDDINGS_PROCESSOR = AutoProcessor.from_pretrained(SIGLIP_MODEL_PATH)

import numpy as np
from more_itertools import chunked

BATCH_SIZE = 32

crops = [sv.cv2_to_pillow(crop) for crop in crops]
batches = chunked(crops, BATCH_SIZE)
data = []
with torch.no_grad():
    for batch in tqdm(batches, desc='embedding extraction'):
        inputs = EMBEDDINGS_PROCESSOR(images=batch, return_tensors="pt").to(DEVICE)
        outputs = EMBEDDINGS_MODEL(**inputs)
        embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()
        data.append(embeddings)

data = np.concatenate(data)

import umap
from sklearn.cluster import KMeans

REDUCER = umap.UMAP(n_components=3)
CLUSTERING_MODEL = KMeans(n_clusters=2)

import supervision as sv
from tqdm import tqdm
from sports.common.team import TeamClassifier

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/Ai League/video.mp4"
PLAYER_ID = 2
STRIDE = 30

frame_generator = sv.get_video_frames_generator(
    source_path=SOURCE_VIDEO_PATH, stride=STRIDE)

crops = []
for frame in tqdm(frame_generator, desc='collecting crops'):
    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
    detections = sv.Detections.from_inference(result)
    players_detections = detections[detections.class_id == PLAYER_ID]
    players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]
    crops += players_crops

team_classifier = TeamClassifier(device="cpu")
team_classifier.fit(crops)

"""# Goalkeeper Detection"""

import numpy as np
import supervision as sv

def resolve_goalkeepers_team_id(
    players: sv.Detections,
    goalkeepers: sv.Detections
) -> np.ndarray:
    goalkeepers_xy = goalkeepers.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
    players_xy = players.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
    team_0_centroid = players_xy[players.class_id == 0].mean(axis=0)
    team_1_centroid = players_xy[players.class_id == 1].mean(axis=0)
    goalkeepers_team_id = []
    for goalkeeper_xy in goalkeepers_xy:
        dist_0 = np.linalg.norm(goalkeeper_xy - team_0_centroid)
        dist_1 = np.linalg.norm(goalkeeper_xy - team_1_centroid)
        goalkeepers_team_id.append(0 if dist_0 < dist_1 else 1)

    return np.array(goalkeepers_team_id)

"""# Load Key points Detection for Pitch from Yolo"""

from inference import get_model
from google.colab import userdata

ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')
FIELD_DETECTION_MODEL_ID = "football-field-detection-f07vi/15"
FIELD_DETECTION_MODEL = get_model(model_id=FIELD_DETECTION_MODEL_ID, api_key=ROBOFLOW_API_KEY)

from sports.annotators.soccer import draw_pitch
from sports.configs.soccer import SoccerPitchConfiguration

CONFIG = SoccerPitchConfiguration()

annotated_frame = draw_pitch(CONFIG)

import supervision as sv
from tqdm import tqdm
from sports.common.team import TeamClassifier

SOURCE_VIDEO_PATH = "/content/drive/MyDrive/Ai League/video.mp4"
PLAYER_ID = 2
STRIDE = 30

frame_generator = sv.get_video_frames_generator(
    source_path=SOURCE_VIDEO_PATH, stride=STRIDE)

crops = []
for frame in tqdm(frame_generator, desc='collecting crops'):
    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
    detections = sv.Detections.from_inference(result)
    players_detections = detections[detections.class_id == PLAYER_ID]
    players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]
    crops += players_crops

team_classifier = TeamClassifier(device="cpu")
team_classifier.fit(crops)

"""# Check the Frame"""

import supervision as sv
import cv2
import numpy as np
from sports.common.view import ViewTransformer
from sports.annotators.soccer import (
    draw_pitch,
    draw_points_on_pitch
)



# Set up video path and IDs
SOURCE_VIDEO_PATH = "/content/drive/MyDrive/Ai League/video.mp4"
BALL_ID, GOALKEEPER_ID, PLAYER_ID, REFEREE_ID = 0, 1, 2, 3

# Annotators for drawing detections
ellipse_annotator = sv.EllipseAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF','#C8102E','#FFD700']),
    thickness=2
)
label_annotator = sv.LabelAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF','#C8102E','#FFD700']),
    text_color=sv.Color.from_hex('#000000'),
    text_position=sv.Position.BOTTOM_CENTER
)
triangle_annotator = sv.TriangleAnnotator(
    color=sv.Color.from_hex('#FFD700'),
    base=20, height=17
)

# Object Tracker
tracker = sv.ByteTrack()
tracker.reset()

# Read frame
frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)
frame = next(frame_generator)

# Object Detection
result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
detections = sv.Detections.from_inference(result)

# Filter detections
ball_detections = detections[detections.class_id == BALL_ID]
ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)

all_detections = detections[detections.class_id != BALL_ID].with_nms(threshold=0.5, class_agnostic=True)
all_detections = tracker.update_with_detections(detections=all_detections)

players_detections = all_detections[all_detections.class_id == PLAYER_ID]
goalkeepers_detections = all_detections[all_detections.class_id == GOALKEEPER_ID]
referees_detections = all_detections[all_detections.class_id == REFEREE_ID]

# Assign team classifications
players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]
players_detections.class_id = team_classifier.predict(players_crops)

goalkeepers_detections.class_id = resolve_goalkeepers_team_id(players_detections, goalkeepers_detections)
referees_detections.class_id -= 1  # Adjust referee class ID

all_detections = sv.Detections.merge([players_detections, goalkeepers_detections, referees_detections])

# Ensure class_id is of integer type before annotation
all_detections.class_id = all_detections.class_id.astype(int)

# Annotate frame
labels = [f"#{tracker_id}" for tracker_id in all_detections.tracker_id]
annotated_frame = frame.copy()
annotated_frame = ellipse_annotator.annotate(annotated_frame, all_detections)
annotated_frame = label_annotator.annotate(annotated_frame, all_detections, labels=labels)
annotated_frame = triangle_annotator.annotate(annotated_frame, ball_detections)

# Field Key Points Detection
result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
key_points = sv.KeyPoints.from_inference(result)

# Perspective Transformation
filter = key_points.confidence[0] > 0.5
frame_reference_points = key_points.xy[0][filter]
pitch_reference_points = np.array(CONFIG.vertices)[filter]

transformer = ViewTransformer(source=frame_reference_points, target=pitch_reference_points)

frame_ball_xy = ball_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
pitch_ball_xy = transformer.transform_points(points=frame_ball_xy)

players_xy = players_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
pitch_players_xy = transformer.transform_points(points=players_xy)

referees_xy = referees_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
pitch_referees_xy = transformer.transform_points(points=referees_xy)

# Draw Radar View
radar_frame = draw_pitch(CONFIG)
radar_frame = draw_points_on_pitch(CONFIG, pitch_ball_xy, face_color=sv.Color.WHITE, edge_color=sv.Color.BLACK, radius=10, pitch=radar_frame)
radar_frame = draw_points_on_pitch(CONFIG, pitch_players_xy[players_detections.class_id == 0], face_color=sv.Color.from_hex('#00BFFF'), edge_color=sv.Color.BLACK, radius=16, pitch=radar_frame)
radar_frame = draw_points_on_pitch(CONFIG, pitch_players_xy[players_detections.class_id == 1], face_color=sv.Color.from_hex('#C8102E'), edge_color=sv.Color.BLACK, radius=16, pitch=radar_frame)
radar_frame = draw_points_on_pitch(CONFIG, pitch_referees_xy, face_color=sv.Color.from_hex('FFD700'), edge_color=sv.Color.BLACK, radius=16, pitch=radar_frame)

# Resize radar
radar_resized = cv2.resize(radar_frame, (800, 500))

# Place radar in bottom-right corner
h, w, _ = annotated_frame.shape
x_offset = w - 810
y_offset = h - 510

# Convert radar to BGR if needed
if radar_resized.shape[2] == 4:
    radar_resized = cv2.cvtColor(radar_resized, cv2.COLOR_RGBA2RGB)

# Blend radar with opacity
alpha = 0.6
overlay = annotated_frame.copy()
overlay[y_offset:y_offset + 500, x_offset:x_offset + 800] = radar_resized
cv2.addWeighted(overlay, alpha, annotated_frame, 1 - alpha, 0, annotated_frame)

# Show final frame
sv.plot_image(annotated_frame)

"""# Final Video Output"""

from tqdm import tqdm
import supervision as sv
import cv2
import numpy as np
from sports.common.view import ViewTransformer
from sports.annotators.soccer import (
    draw_pitch,
    draw_points_on_pitch
)

# Set up video path and IDs
SOURCE_VIDEO_PATH = "/content/drive/MyDrive/Ai League/video.mp4"
TARGET_VIDEO_PATH = "/content/drive/MyDrive/Ai League/Result_final.mp4"

BALL_ID, GOALKEEPER_ID, PLAYER_ID, REFEREE_ID = 0, 1, 2, 3

# Annotators for drawing detections
ellipse_annotator = sv.EllipseAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF','#C8102E','#FFD700']),
    thickness=2
)
label_annotator = sv.LabelAnnotator(
    color=sv.ColorPalette.from_hex(['#00BFFF','#C8102E','#FFD700']),
    text_color=sv.Color.from_hex('#000000'),
    text_position=sv.Position.BOTTOM_CENTER
)
triangle_annotator = sv.TriangleAnnotator(
    color=sv.Color.from_hex('#FFD700'),
    base=20, height=17
)

# Object Tracker
tracker = sv.ByteTrack()
tracker.reset()

# Read frame
video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)
video_sink = sv.VideoSink(TARGET_VIDEO_PATH, video_info=video_info)
frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)

with video_sink:
    for frame in tqdm(frame_generator, total=video_info.total_frames):
        # Object Detection
        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
        detections = sv.Detections.from_inference(result)

        # Filter detections
        ball_detections = detections[detections.class_id == BALL_ID]
        ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)

        all_detections = detections[detections.class_id != BALL_ID].with_nms(threshold=0.5, class_agnostic=True)
        all_detections = tracker.update_with_detections(detections=all_detections)

        # Ensure `tracker_id` and `class_id` are valid
        if all_detections.tracker_id is not None:
            valid_indices = ~np.isnan(all_detections.tracker_id)  # Remove NaN tracker IDs
            all_detections = all_detections[valid_indices]
            all_detections.tracker_id = all_detections.tracker_id.astype(int)

        players_detections = all_detections[all_detections.class_id == PLAYER_ID]
        goalkeepers_detections = all_detections[all_detections.class_id == GOALKEEPER_ID]
        referees_detections = all_detections[all_detections.class_id == REFEREE_ID]

        # Assign team classifications
        players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]
        players_detections.class_id = team_classifier.predict(players_crops)

        goalkeepers_detections.class_id = resolve_goalkeepers_team_id(players_detections, goalkeepers_detections)
        referees_detections.class_id -= 1  # Adjust referee class ID

        # Ensure `class_id` is integer
        all_detections = sv.Detections.merge([players_detections, goalkeepers_detections, referees_detections])
        all_detections.class_id = all_detections.class_id.astype(int)

        # Annotate frame
        labels = [f"#{int(tracker_id)}" for tracker_id in all_detections.tracker_id]  # Convert IDs to int
        annotated_frame = frame.copy()
        annotated_frame = ellipse_annotator.annotate(annotated_frame, all_detections)
        annotated_frame = label_annotator.annotate(annotated_frame, all_detections, labels=labels)
        annotated_frame = triangle_annotator.annotate(annotated_frame, ball_detections)

        # Field Key Points Detection
        result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]
        key_points = sv.KeyPoints.from_inference(result)

        # Perspective Transformation
        filter = key_points.confidence[0] > 0.5
        frame_reference_points = key_points.xy[0][filter]
        pitch_reference_points = np.array(CONFIG.vertices)[filter]

        transformer = ViewTransformer(source=frame_reference_points, target=pitch_reference_points)

        frame_ball_xy = ball_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
        pitch_ball_xy = transformer.transform_points(points=frame_ball_xy)

        players_xy = players_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
        pitch_players_xy = transformer.transform_points(points=players_xy)

        referees_xy = referees_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)
        pitch_referees_xy = transformer.transform_points(points=referees_xy)

        # Draw Radar View
        radar_frame = draw_pitch(CONFIG)
        radar_frame = draw_points_on_pitch(CONFIG, pitch_ball_xy, face_color=sv.Color.WHITE, edge_color=sv.Color.BLACK, radius=10, pitch=radar_frame)
        radar_frame = draw_points_on_pitch(CONFIG, pitch_players_xy[players_detections.class_id == 0], face_color=sv.Color.from_hex('#00BFFF'), edge_color=sv.Color.BLACK, radius=16, pitch=radar_frame)
        radar_frame = draw_points_on_pitch(CONFIG, pitch_players_xy[players_detections.class_id == 1], face_color=sv.Color.from_hex('#C8102E'), edge_color=sv.Color.BLACK, radius=16, pitch=radar_frame)
        radar_frame = draw_points_on_pitch(CONFIG, pitch_referees_xy, face_color=sv.Color.from_hex('FFD700'), edge_color=sv.Color.BLACK, radius=16, pitch=radar_frame)

        # Resize radar
        radar_resized = cv2.resize(radar_frame, (800, 500))

        # Place radar in bottom-right corner
        h, w, _ = annotated_frame.shape
        x_offset = w - 810
        y_offset = h - 510

        # Convert radar to BGR if needed
        if radar_resized.shape[2] == 4:
            radar_resized = cv2.cvtColor(radar_resized, cv2.COLOR_RGBA2RGB)

        # Blend radar with opacity
        alpha = 0.6
        overlay = annotated_frame.copy()
        overlay[y_offset:y_offset + 500, x_offset:x_offset + 800] = radar_resized
        cv2.addWeighted(overlay, alpha, annotated_frame, 1 - alpha, 0, annotated_frame)

        # Write frame to video
        video_sink.write_frame(annotated_frame)

