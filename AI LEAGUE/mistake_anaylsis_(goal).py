# -*- coding: utf-8 -*-
"""Mistake_anaylsis_(Goal).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fLqbXjPm-P_TNjylw3xZlZoEtyOrLSll
"""



!pip install transformers accelerate av torch numpy -q

import av
import torch
import numpy as np
from transformers import VideoLlavaForConditionalGeneration, VideoLlavaProcessor

def read_video_pyav(container, indices):
    '''
    Decode the video with PyAV decoder.
    Args:
        container (`av.container.input.InputContainer`): PyAV container.
        indices (`List[int]`): List of frame indices to decode.
    Returns:
        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).
    '''
    frames = []
    container.seek(0)
    start_index = indices[0]
    end_index = indices[-1]
    for i, frame in enumerate(container.decode(video=0)):
        if i > end_index:
            break
        if i >= start_index and i in indices:
            frames.append(frame)
    return np.stack([x.to_ndarray(format="rgb24") for x in frames])

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

video_path = "/content/drive/MyDrive/video/Video.mp4"

container = av.open(video_path)
total_frames = container.streams.video[0].frames

indices = np.linspace(0, total_frames - 1, num=4, dtype=int)
video = read_video_pyav(container, indices)

model = VideoLlavaForConditionalGeneration.from_pretrained(
    "LanguageBind/Video-LLaVA-7B-hf",
    torch_dtype=torch.float16,
    device_map="auto",
    offload_folder="offload",
    offload_buffers=True
)

model = torch.compile(model)

processor = VideoLlavaProcessor.from_pretrained("LanguageBind/Video-LLaVA-7B-hf")

prompt = """USER: <video>
Analyze the defensive mistakes in this video that led to the goal.
- Identify any errors in positioning.
- Evaluate goalkeeper reactions and defenders' roles.
- Suggest improvements to prevent such goals.

ASSISTANT:"""
inputs = processor(text=prompt, videos=video, return_tensors="pt").to(device)

out = model.generate(**inputs, max_new_tokens=250)

print(processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True))

