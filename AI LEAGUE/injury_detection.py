# -*- coding: utf-8 -*-
"""injury_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11e72A-iQin2bUoLniLNR6SPOrtiVIiqL
"""

!pip install transformers accelerate av torch numpy -q

import av
import torch
import numpy as np
from transformers import VideoLlavaForConditionalGeneration, VideoLlavaProcessor

def load_model_and_processor():
    model = VideoLlavaForConditionalGeneration.from_pretrained(
        "LanguageBind/Video-LLaVA-7B-hf",
        torch_dtype=torch.float16,
        device_map="auto",
        offload_folder="offload",
        offload_buffers=True
    )

    # Optional: Use torch.compile (ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø²Ø§Ù„ØªÙ‡ Ù„Ùˆ Ø¹Ù…Ù„ Ù…Ø´Ø§ÙƒÙ„)
    model = torch.compile(model)

    processor = VideoLlavaProcessor.from_pretrained("LanguageBind/Video-LLaVA-7B-hf")

    return model, processor

def read_video_pyav(container, indices):
    frames = []
    container.seek(0)
    start_index = indices[0]
    end_index = indices[-1]
    for i, frame in enumerate(container.decode(video=0)):
        if i > end_index:
            break
        if i >= start_index and i in indices:
            frames.append(frame)
    return np.stack([x.to_ndarray(format="rgb24") for x in frames])

def analyze_injury(video_path, model, processor):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    container = av.open(video_path)
    total_frames = container.streams.video[0].frames
    indices = np.linspace(0, total_frames - 1, num=4, dtype=int)
    video = read_video_pyav(container, indices)

    prompt = """USER: <video>
You are an expert AI analyst reviewing a football match injury video.

Please watch the clip carefully and extract detailed information about the injury.

ğŸ” Your task is to analyze the video and return the information in a structured table format with the following columns:

| Injury Description | Injury Severity | Medical Team Involved | Was Play Stopped? | Injury Location | Did the Player Leave the Field? |
|--------------------|------------------|------------------------|--------------------|------------------|-------------------------------|

ğŸ”¸ Column details:
- **Injury Description**: Briefly describe what happened in 2â€“3 sentences (e.g., "The player fell after a heavy tackle from the opponent and showed signs of pain in his left knee.").
- **Injury Severity**: Choose from [Minor, Moderate, Severe].
- **Medical Team Involved**: Yes / No â€” Did the medical team enter the field?
- **Was Play Stopped?**: Yes / No â€” Was the match interrupted due to the injury?
- **Injury Location**: Be specific (e.g., right ankle, left shoulder, head).
- **Did the Player Leave the Field?**: Yes / No â€” Did the player continue playing or leave?

ğŸ“Œ Return only the completed table in your response.

--- Example Output:

| Injury Description                                               | Injury Severity | Medical Team Involved | Was Play Stopped? | Injury Location | Did the Player Leave the Field? |
|------------------------------------------------------------------|------------------|------------------------|--------------------|------------------|-------------------------------|
| The player collapsed after a strong challenge on his right leg. He was visibly in pain and held his shin for several seconds. The medical team quickly came to assess the injury. | Moderate         | Yes                    | Yes                | Right shin       | No                            |

ASSISTANT:"""

    inputs = processor(text=prompt, videos=video, return_tensors="pt").to(device)
    out = model.generate(**inputs, max_new_tokens=250)
    result = processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)

    return result

video_path = "/content/drive/MyDrive/Ai League/injury.mp4"

# Ø­Ù…Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
model, processor = load_model_and_processor()

# Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­Ù„ÙŠÙ„ Ø£ÙŠ ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¯ÙˆÙ† ØªØ­Ù…ÙŠÙ„Ù‡ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰
result = analyze_injury(video_path, model, processor)

print(result)

import re

def extract_model_response(result):
    """
    Extracts the model's response, specifically the table data, from the output string.

    Args:
        result (List[str]): The output from processor.batch_decode, e.g., [output_text]

    Returns:
        str: The extracted model's table response
    """
    if isinstance(result, list):
        output_text = result[0]
    else:
        output_text = result

    # Extract the part of the response that contains the table data
    match = re.search(r"ASSISTANT:\s*(\|.*\|)", output_text, re.DOTALL)

    if match:
        return match.group(1).strip()
    else:
        print("âš ï¸ Unable to extract the model's table response.")
        return None

# Example usage
model_response = extract_model_response(result)
print(model_response)









